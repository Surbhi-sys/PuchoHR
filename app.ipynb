{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c3d1e31",
   "metadata": {},
   "source": [
    "#import guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85fe4acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from hr__guardrails import (\n",
    "    violates_safety_policy,\n",
    "    is_hr_question,\n",
    "    is_answer_grounded,\n",
    "    contains_sensitive_advice,\n",
    "    is_query_too_long\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627f0ac",
   "metadata": {},
   "source": [
    "Load PDFs (with OCR fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "985468b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91880\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages: 72\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredPDFLoader\n",
    "\n",
    "def load_all_pdfs(pdf_folder_path):\n",
    "    documents = []\n",
    "\n",
    "    for file_name in os.listdir(pdf_folder_path):\n",
    "        if not file_name.endswith(\".pdf\"):\n",
    "            continue\n",
    "\n",
    "        full_path = os.path.join(pdf_folder_path, file_name)\n",
    "\n",
    "        loader = PyPDFLoader(full_path)\n",
    "        pages = loader.load()\n",
    "\n",
    "        extracted_text = \"\".join([p.page_content.strip() for p in pages])\n",
    "\n",
    "        if not extracted_text:\n",
    "            print(f\"OCR applied for: {file_name}\")\n",
    "            loader = UnstructuredPDFLoader(full_path, strategy=\"ocr_only\")\n",
    "            pages = loader.load()\n",
    "\n",
    "        for doc in pages:\n",
    "            doc.metadata[\"source\"] = file_name\n",
    "\n",
    "        documents.extend(pages)\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "pdf_folder_path = r\"C:\\Users\\91880\\Desktop\\GenAI_Projects\\PuchoHR\\Data\\documents\"\n",
    "docs = load_all_pdfs(pdf_folder_path)\n",
    "print(\"Total pages:\", len(docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68352d13",
   "metadata": {},
   "source": [
    "Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "606a694a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 526\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=256\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "print(\"Total chunks:\", len(chunks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0d963a",
   "metadata": {},
   "source": [
    "Create & Persist Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de9a0cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB created and persisted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91880\\AppData\\Local\\Temp\\ipykernel_16376\\3096279755.py:12: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vector_db.persist()\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embedding = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "vector_db.persist()\n",
    "print(\"Vector DB created and persisted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240af5d",
   "metadata": {},
   "source": [
    "Load Vector DB + Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dee16a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91880\\AppData\\Local\\Temp\\ipykernel_16376\\1718535300.py:6: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  vector_db = Chroma(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "vector_db = Chroma(\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    embedding_function=embedding\n",
    ")\n",
    "\n",
    "llm = OllamaLLM(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5853b1e0",
   "metadata": {},
   "source": [
    "HR Bot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf9d135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_hr(question):\n",
    "    if is_query_too_long(question):\n",
    "        return \"Your query is too long, please shorten it.\"\n",
    "\n",
    "    if violates_safety_policy(question):\n",
    "        return \"Sorry, I cannot answer that question due to safety policy.\"\n",
    "\n",
    "    if not is_hr_question(question):\n",
    "        return \"Sorry, this question is not related to HR policies.\"\n",
    "\n",
    "    docs = vector_db.similarity_search(question, k=4)\n",
    "    if not docs:\n",
    "        return \"Information not found in documents.\"\n",
    "\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"Source: {doc.metadata['source']}\\n{doc.page_content}\"\n",
    "        for doc in docs\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an HR assistant.\n",
    "Answer ONLY using the provided context.\n",
    "If the answer is not present, say:\n",
    "\"Information not found in documents.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    answer = llm.invoke(prompt)\n",
    "\n",
    "    if not is_answer_grounded(answer, docs):\n",
    "        return \"Information not found in documents.\"\n",
    "\n",
    "    answer_emb = embedding_model.encode(str(answer))\n",
    "    context_text = \" \".join(doc.page_content for doc in docs)\n",
    "    context_emb = embedding_model.encode(context_text)\n",
    "    similarity = util.cos_sim(answer_emb, context_emb).item()\n",
    "\n",
    "    if similarity < 0.8:\n",
    "        return \"Answer may not be fully accurate based on available documents.\"\n",
    "\n",
    "    if contains_sensitive_advice(answer):\n",
    "        return \"I cannot provide legal or medical advice. Please consult the appropriate professional.\"\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630fb07f",
   "metadata": {},
   "source": [
    "Database + Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2240a32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM answer: Answer may not be fully accurate based on available documents.\n",
      "[('What is the leave policy for regular employees?', 'Answer may not be fully accurate based on available documents.'), ('What is the leave policy for regular employees?', 'According to the HR Policy-GGIAL.pdf, the leave policies for regular employees are as follows:\\n\\n* Sick Leave: 6 days per year; Medical Proof of leave exceeding 2 days must be submitted by the employee. No limit on accumulation, but it cannot be encashed. SL can be availed for half day also.\\n* Privilege Leave: 24 days per year; Maximum Accumulation is up to 180 days. PL can be encased only at the time of separation, and monthly basic salary will be considered for calculation.')]\n"
     ]
    }
   ],
   "source": [
    "from db_util import init_db, store_chat, get_recent_chat\n",
    "\n",
    "init_db()\n",
    "\n",
    "session_id = \"user123\"\n",
    "question = \"What is the leave policy for regular employees?\"\n",
    "\n",
    "answer = ask_hr(question)\n",
    "print(\"LLM answer:\", answer)\n",
    "\n",
    "store_chat(session_id, question, answer, source_doc=\"HR_Policy-GGIAL.pdf\", similarity_score=0.92)\n",
    "\n",
    "previous_chats = get_recent_chat(session_id)\n",
    "print(previous_chats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64bef69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: Answer may not be fully accurate based on available documents.\n",
      "Q2: Information not found in documents.\n",
      "\n",
      "Recent Chat History:\n",
      "Q: What is the probation period?\n",
      "A: Information not found in documents.\n",
      "--------------------------------------------------\n",
      "Q: What is the leave policy for regular employees?\n",
      "A: Answer may not be fully accurate based on available documents.\n",
      "--------------------------------------------------\n",
      "Q: What is the leave policy for regular employees?\n",
      "A: Answer may not be fully accurate based on available documents.\n",
      "--------------------------------------------------\n",
      "Q: What is the leave policy for regular employees?\n",
      "A: According to the HR Policy-GGIAL.pdf, the leave policies for regular employees are as follows:\n",
      "\n",
      "* Sick Leave: 6 days per year; Medical Proof of leave exceeding 2 days must be submitted by the employee. No limit on accumulation, but it cannot be encashed. SL can be availed for half day also.\n",
      "* Privilege Leave: 24 days per year; Maximum Accumulation is up to 180 days. PL can be encased only at the time of separation, and monthly basic salary will be considered for calculation.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Ask multiple questions\n",
    "q1 = \"What is the leave policy for regular employees?\"\n",
    "a1 = ask_hr(q1)\n",
    "store_chat(\"user123\", q1, a1, source_doc=\"HR_Policy-GGIAL.pdf\", similarity_score=0.90)\n",
    "\n",
    "q2 = \"What is the probation period?\"\n",
    "a2 = ask_hr(q2)\n",
    "store_chat(\"user123\", q2, a2, source_doc=\"HR_Policy-GGIAL.pdf\", similarity_score=0.88)\n",
    "\n",
    "print(\"Q1:\", a1)\n",
    "print(\"Q2:\", a2)\n",
    "\n",
    "# Fetch last chats\n",
    "history = get_recent_chat(\"user123\", limit=5)\n",
    "print(\"\\nRecent Chat History:\")\n",
    "for q, a in history:\n",
    "    print(\"Q:\", q)\n",
    "    print(\"A:\", a)\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
